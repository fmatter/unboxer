{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting started","text":"<p>The unboxer is a tool to turn toolbox and shoebox databases into CSV files. Its primary focus is the creation of CLDF datasets, but the extracted files can be used in other ways, too.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install the unboxer python library:</p> <pre><code>pip install unboxer\n</code></pre>"},{"location":"#problems","title":"Problems","text":"<ol> <li>if running <code>unbox</code> produces warnings, check your database for possible inconsistencies</li> <li>if that did not solve your problem, open an issue</li> </ol>"},{"location":"api/","title":"Python API","text":"<p>Top-level package for unboxer.</p>"},{"location":"api/#unboxer.extract_corpus","title":"<code>extract_corpus(filenames=None, conf=None, lexicon=None, output_dir='.', cldf=False, audio=None, skip_empty_obj=False, complain=False, segments=None, inflection=None, include=None, parsing=None, languages=None)</code>","text":"<p>Extract text records from a corpus.</p> <p>Parameters:</p> Name Type Description Default <code>database_file</code> <code>str</code> <p>The path to the corpus database file.</p> required <code>conf</code> <code>dict</code> <p>Configuration (see) todo: insert link</p> <code>None</code> <code>cldf</code> <code>bool</code> <p>Should a CLDF dataset be created? Defaults to <code>False</code>.</p> <code>False</code> Source code in <code>src/unboxer/__init__.py</code> <pre><code>def extract_corpus(\n    filenames=None,\n    conf=None,\n    lexicon=None,\n    output_dir=\".\",\n    cldf=False,\n    audio=None,\n    skip_empty_obj=False,\n    complain=False,\n    segments=None,\n    inflection=None,\n    include=None,\n    parsing=None,\n    languages=None,\n):\n    \"\"\"Extract text records from a corpus.\n\n    Args:\n        database_file (str): The path to the corpus database file.\n        conf (dict): Configuration (see) todo: insert link\n        cldf (bool, optional): Should a CLDF dataset be created? Defaults to `False`.\n    \"\"\"\n    file_recs = {}\n    inflection = inflection or {}\n    output_dir.mkdir(exist_ok=True, parents=True)\n    for filename in filenames:\n        database_file = Path(filename)\n        record_marker = \"\\\\\" + conf[\"record_marker\"]\n        sep = conf[\"cell_separator\"]\n\n        try:\n            with open(database_file, \"r\", encoding=conf[\"encoding\"]) as f:\n                content = f.read()\n        except UnicodeDecodeError:\n            log.error(\n                f\"\"\"Could not open the file with the encoding [{conf[\"encoding\"]}].\n    Make sure that you are not parsing a shoebox project as toolbox or vice versa.\n    You can also explicitly set the correct file encoding in your config.\"\"\"\n            )\n            sys.exit()\n        file_recs[filename] = []\n        records = content.split(record_marker + \" \")\n        for record in records[1::]:\n            res = _get_fields(\n                record_marker + \" \" + record, record_marker, multiple=[], sep=sep\n            )\n            if res:\n                file_recs[filename].append(res)\n            else:\n                pass\n                # log.warning(\"Empty record:\")\n                # log.warning(record)\n    dfs = {x: pd.DataFrame.from_dict(y) for x, y in file_recs.items()}\n    all_texts = []\n    for fn, df in dfs.items():\n        log.info(f\"Processing {fn}\")\n        if conf[\"text_mode\"] != \"none\":\n            text_path = output_dir / f\"{fn.stem}_texts.csv\"\n            if text_path.is_file():\n                texts = load(text_path)\n            else:\n                texts = []\n\n        if record_marker in df and conf.get(\"slugify\", True):\n            if conf[\"interlinear_mappings\"].get(record_marker, \"\") == \"ID\":\n                conf[\"interlinear_mappings\"].pop(record_marker)\n            tqdm.pandas(desc=\"Creating record IDs\")\n            df[\"ID\"] = df[record_marker].progress_apply(\n                lambda x: humidify(x, \"sentence_id\", unique=True)\n            )\n        else:\n            df[\"ID\"] = df.index\n        df[\"filename\"] = fn.name\n\n        if conf[\"text_mode\"] == \"record_marker\":\n            tmap_file = output_dir / f\"{fn.stem}_textmap.yaml\"\n            if tmap_file.is_file():\n                text_map = load(tmap_file)\n            elif \"ID\" in df.columns:\n                text_map = guess_texts(list(df[\"ID\"]), fn)\n                dump(text_map, tmap_file)\n                log.info(\n                    f\"Created tentative record-text mapping in {tmap_file.resolve()}\"\n                )\n            else:\n                text_map = {}\n            if isinstance(texts, list):\n                texts.extend(text_map.keys())\n                texts = pd.DataFrame(texts)\n                texts.columns = [\"ID\"]\n                for addcol in [\"Name\", \"Description\", \"Comment\", \"Source\", \"Type\"]:\n                    texts[addcol] = \"\"\n                dump(texts, text_path)\n            reverse_map = {}\n            for text_id, recs in text_map.items():\n                for rec in recs:\n                    reverse_map[rec] = text_id\n            df[\"Text_ID\"] = df[\"ID\"].map(reverse_map).fillna(\"\")\n            all_texts.append(texts)\n    df = pd.concat(dfs.values())\n    if not df[record_marker].is_unique:\n        if complain:\n            log.warning(\"Found duplicate IDs, will only keep first of each:\")\n            dupes = df[df.duplicated(record_marker)]\n            print(dupes)\n        df.drop_duplicates(record_marker, inplace=True)\n    df.rename(columns=conf[\"interlinear_mappings\"], inplace=True)\n    if \"Analyzed_Word\" not in df.columns:\n        raise ValueError(\"Did not find Analyzed_Word:\", conf[\"interlinear_mappings\"])\n    if conf[\"skip_empty_obj\"]:\n        old = len(df)\n        df = df[df[\"Gloss\"] != \"\"]\n        log.info(f\"Dropped {old-len(df)} unparsed records.\")\n    df.fillna(\"\", inplace=True)\n    df = df[df[\"Primary_Text\"] != \"\"]\n\n    if lexicon:\n        lex_df = extract_lexicon(\n            lexicon, parsing=parsing, conf=conf, output_dir=output_dir\n        )\n        morphemes, morphs = extract_morphs(lex_df, sep)\n        morphinder = Morphinder(morphs, complain=complain)\n    else:\n        tdf = df.copy()\n        morphs = {}\n        for c in [\"Analyzed_Word\", \"Gloss\"]:\n            tdf[c] = df[c].apply(lambda x: re.sub(r\"-\\s+\", \"-INTERN\", x))\n            tdf[c] = df[c].apply(lambda x: re.sub(r\"\\s+-\", \"INTERN-\", x))\n            tdf[c] = df[c].apply(lambda x: re.split(r\"\\s+\", x))\n        for rec in tdf.to_dict(\"records\"):\n            for obj, gloss in zip(rec[\"Analyzed_Word\"], rec[\"Gloss\"]):\n                if obj == \"\":\n                    continue\n                morph_id = humidify(obj + \"-\" + gloss, key=\"pairs\")\n                if morph_id not in morphs:\n                    morphs[morph_id] = {\n                        \"ID\": morph_id,\n                        \"Form\": obj,\n                        \"Meaning\": gloss.strip(\"-\").strip(\"=\"),\n                    }\n        morphs = pd.DataFrame.from_dict(morphs.values())\n        morphinder = Morphinder(morphs, complain=complain)\n    (\n        wordforms,\n        form_meanings,\n        sentence_slices,\n        morph_slices,\n        inflections,\n        stems,\n        wordformstems,\n        stemparts,\n    ) = build_slices(df, morphinder, **inflection)\n    morph_meanings = {}\n    stem_meanings = {}\n    for meanings in tqdm(morphs[\"Meaning\"], desc=\"Morphs\"):\n        for meaning in meanings.split(\"; \"):\n            morph_meanings.setdefault(\n                meaning, {\"ID\": humidify(meaning, key=\"meanings\"), \"Name\": meaning}\n            )\n\n    if len(stems) &gt; 0:\n        for stem_gloss in tqdm(stems[\"Meaning\"], desc=\"Stems\"):\n            stem_meanings.setdefault(\n                stem_gloss,\n                {\n                    \"ID\": humidify(stem_gloss, key=\"meanings\"),\n                    \"Name\": stem_gloss,\n                },\n            )\n    if include:\n        include = load(include)\n        rec_list = include\n    else:\n        rec_list = list(df[\"ID\"])\n    df = df[df[\"ID\"].isin(rec_list)]\n\n    if conf[\"text_mode\"] != \"none\":\n        texts = pd.concat(all_texts)\n        texts = texts[texts[\"ID\"].isin(list(df[\"Text_ID\"]))]\n\n    sentence_slices = sentence_slices[sentence_slices[\"Example_ID\"].isin(rec_list)]\n    for col in tqdm(df.columns, desc=\"Columns\"):\n        if col in conf[\"aligned_fields\"]:\n            df[col] = df[col].apply(_remove_spaces)\n    df = df.apply(helpers.fix_glosses, axis=1)\n    sentence_slices = sentence_slices[sentence_slices[\"Example_ID\"].isin(rec_list)]\n    if conf[\"fix_clitics\"]:\n        log.info(\"Fixing clitics\")\n        for col in conf[\"aligned_fields\"]:\n            df[col] = df[col].apply(_fix_clitics)\n    if \"Primary_Text\" in df.columns:\n        df[\"Primary_Text\"] = df[\"Primary_Text\"].apply(lambda x: re.sub(r\"\\s+\", \" \", x))\n\n    if len(wordforms) &gt; 0:\n        wordforms = wordforms[wordforms[\"Form\"] != \"\"]\n\n    for x in [df, wordforms, morphs]:\n        x[\"Language_ID\"] = conf.get(\"lang_id\", \"undefined\")\n    if lexicon:\n        morphemes[\"Language_ID\"] = conf.get(\"lang_id\", \"undefined\")\n    if not morphs[\"ID\"].is_unique:\n        log.warning(\"Duplicate IDs in morph table, only keeping first instances:\")\n        log.warning(morphs[morphs.duplicated(subset=\"ID\", keep=False)])\n        morphs.drop_duplicates(subset=\"ID\", inplace=True)\n    if output_dir:\n        df.to_csv(\n            (Path(output_dir) / database_file.name).with_suffix(\".csv\"), index=False\n        )\n        morphs.to_csv((Path(output_dir) / \"morphs.csv\"), index=False)\n        if lexicon:\n            morphemes.to_csv((Path(output_dir) / \"morphemes.csv\"), index=False)\n    if cldf:\n        tables = {\"examples.csv\": df}\n        tables[\"exampleparts.csv\"] = sentence_slices\n        if lexicon:\n            morphemes[\"Name\"] = morphemes[\"Headword\"]\n            morphemes[\"Description\"] = morphemes[\"Meaning\"]\n            morphemes[\"Parameter_ID\"] = morphemes[\"Meaning\"].apply(\n                lambda x: [morph_meanings[y][\"ID\"] for y in x.split(\"; \")]\n            )\n        if inflection:\n            stems[\"Parameter_ID\"] = stems[\"Meaning\"].apply(\n                lambda x: [stem_meanings[x][\"ID\"]]\n            )\n\n        if audio:\n            tables[\"media.to_csv\"] = pd.DataFrame.from_dict(\n                [\n                    {\n                        \"ID\": f.stem,\n                        \"Media_Type\": \"audio/\" + f.suffix.strip(\".\"),\n                        \"Download_URL\": str(f),\n                    }\n                    for f in audio.iterdir()\n                ]\n            )\n\n        morphs[\"Name\"] = morphs[\"Form\"]\n        if segments:\n            extra = [\"+\", \"-\", \"(\", \")\", \"/\", \"\u2205\", \"0\", \"?\", \",\", \"=\", \";\"]\n            pdf = load(segments)\n            tokenizer = Tokenizer(\n                Profile(\n                    *(\n                        pdf.to_dict(\"records\")\n                        + [{\"Grapheme\": x, \"IPA\": x} for x in extra]\n                    )\n                )\n            )\n            log.info(\"Tokenizing...\")\n            tokenize = lambda x: tokenizer(x.lower().replace(\"-\", \"\"), column=\"IPA\")\n            for m_df in [wordforms, morphs]:\n                if len(m_df) &gt; 0:\n                    for orig, repl in conf.get(\"replace\", {}).items():\n                        m_df[\"Form\"] = m_df[\"Form\"].replace(orig, repl, regex=True)\n                    m_df[\"Segments\"] = m_df[\"Form\"].apply(\n                        lambda x: tokenize(x).split(\" \")\n                    )\n                    bad = m_df[m_df[\"Segments\"].apply(lambda x: \"\ufffd\" in x)]\n                    if len(bad) &gt; 1:\n                        log.warning(f\"Unsegmentable: &lt;{bad}&gt;\")\n                        m_df[\"Segments\"] = m_df[\"Segments\"].apply(\n                            lambda x: \"\" if \"\ufffd\" in x else x\n                        )\n        if len(morph_slices) &gt; 0:\n            morph_slices[\"Gloss_ID\"] = morph_slices[\"Gloss\"].apply(id_glosses)\n            tables[\"glosses.csv\"] = pd.DataFrame.from_dict(\n                [{\"ID\": v, \"Name\": k} for k, v in get_values(\"glosses\").items()]\n            )\n        morphs[\"Description\"] = morphs[\"Meaning\"]\n        morphs[\"Parameter_ID\"] = morphs[\"Description\"].apply(\n            lambda x: [morph_meanings[y][\"ID\"] for y in x.split(\"; \")]\n        )\n        if len(form_meanings) &gt; 0:\n            morph_meanings = pd.DataFrame.from_dict(\n                [\n                    x\n                    for x in morph_meanings.values()\n                    if x[\"ID\"] not in list(form_meanings[\"ID\"])\n                ]\n            )\n            stem_meanings = pd.DataFrame.from_dict(\n                [\n                    x\n                    for x in stem_meanings.values()\n                    if x[\"ID\"] not in list(form_meanings[\"ID\"])\n                ]\n            )\n            tables[\"parameters.csv\"] = pd.concat(\n                [form_meanings, morph_meanings, stem_meanings]\n            )\n        else:\n            morph_meanings = pd.DataFrame.from_dict(morph_meanings.values())\n            tables[\"parameters.csv\"] = morph_meanings\n        if len(wordforms) &gt; 0:\n            tables[\"wordforms.csv\"] = wordforms\n        tables[\"morphs.csv\"] = morphs\n        tables[\"wordformparts.csv\"] = morph_slices\n        if len(stems) &gt; 0:\n            stems[\"Language_ID\"] = conf.get(\"lang_id\", \"undefined\")\n            stems[\"Lexeme_ID\"] = stems[\"ID\"]\n            tables[\"stems.csv\"] = stems\n            tables[\"lexemes.csv\"] = stems\n            tables[\"stemparts.csv\"] = stemparts\n            tables[\"wordformstems.csv\"] = wordformstems\n            tables[\"inflections.csv\"] = inflections\n            tables[\"inflectionalcategories.csv\"] = inflection[\"infl_cats\"]\n            tables[\"inflectionalvalues.csv\"] = inflection[\"infl_vals\"]\n        if conf[\"text_mode\"] != \"none\" and len(texts) &gt; 0 and len(df) &gt; 0:\n            tables[\"texts.csv\"] = texts\n        if lexicon:\n            lexicon, meanings = get_lexical_data(lex_df)\n            tables[\"morphemes.csv\"] = morphemes\n            tables[\"parameters.csv\"] = pd.concat([meanings, tables[\"parameters.csv\"]])\n            tables[\"parameters.csv\"].drop_duplicates(subset=\"ID\", inplace=True)\n        create_cldf(\n            tables=tables,\n            conf=conf,\n            output_dir=output_dir,\n            cldf_name=conf.get(\"cldf_name\", \"cldf\"),\n            languages=languages,\n            module=\"corpus\",\n        )\n    return df\n</code></pre>"},{"location":"config/","title":"Configuration","text":"<p>The unboxer's behavior can be modified by editing a .yaml file and passing it to the <code>unbox</code> command:</p> <pre><code>unbox corpus &lt;path/to/your/toolbox/texts.db&gt; --conf &lt;path/to/your/config.yaml&gt;`.\n</code></pre> <p>A key-value pair added in your configuration file will override the default value. Default values are stored in one of three built-in  files. There is a general configuration file for interlinear text, and specific files for toolbox and shoebox. These two files are identically structured and contain best-guess default values for the two applications. By default, the toolbox configuration is loaded; shoebox can be specified with <code>--format shoebox</code>.</p>"},{"location":"config/#general-configuration","title":"General configuration","text":"<p>File: <code>interlinear_config.yaml</code>.</p> <p>This configuration file contains the default values for variables related to interlinear text.</p>"},{"location":"config/#tab-aligned-fields","title":"Tab-aligned fields","text":"<p>A list of fields containing text which will be aligned when rendered in an interlinear representation.  Note that you need to use the field labels present after renaming, see here.</p> <ul> <li><code>Analyzed_Word</code>: cldf#Analyzed_Word</li> <li><code>Gloss</code>: cldf#Gloss</li> <li>Part of speech (<code>Part_Of_Speech</code>): currently specified as a dictionary entry property in CLDF, usable as a foreign key in cldf-ldd, with a corresponding component.</li> </ul> Default:<pre><code>aligned_fields:\n    - Analyzed_Word\n    - Gloss\n    - Part_Of_Speech\n</code></pre>"},{"location":"config/#slugification","title":"Slugification","text":"<p>Turn record IDs (<code>\\ref</code>) into database-usable IDs, e.g. <code>ConvInGarden.003</code> into <code>convingarden-003</code>.</p> Default:<pre><code>slugify: true\n</code></pre>"},{"location":"config/#clitic-space-correction","title":"Clitic space correction","text":"<p>Remove spaces after proclitics and before enclitics.</p> Default:<pre><code>fix_clitics: true\n</code></pre>"},{"location":"config/#cell-separator","title":"Cell separator","text":"<p>How multiple values in a cell (like variants, or meanings) are delimited.</p> Default:<pre><code>cell_separator: '; '\n</code></pre>"},{"location":"config/#skip-empty-records","title":"Skip empty records","text":"Default:<pre><code>skip_empty_obj: true\n</code></pre>"},{"location":"config/#toolbox","title":"Toolbox","text":"<p>File: <code>toolbox.yaml</code>.</p> <p>This is the default configuration for toolbox projects.</p>"},{"location":"config/#file-encoding","title":"File encoding","text":"<p>toolbox files should be in UTF-8.</p> Default:<pre><code>encoding: 'utf-8'\n</code></pre>"},{"location":"config/#text-record-marker","title":"Text record marker","text":"<p>The field holding text record identifiers.</p> Default:<pre><code>record_marker: 'ref'\n</code></pre>"},{"location":"config/#mapping-interlinear-fields-to-columns","title":"Mapping interlinear fields to columns","text":"<p>The fields listed here will be renamed accordingly.  Note that all field markers are represented here without the leading <code>\\</code> that is used in toolbox.</p>"},{"location":"config/#unsegmented-object-line","title":"Unsegmented object line","text":"<p>The unsegmented first line of the record.</p> Default:<pre><code>interlinear_mappings:\n  tx: 'Primary_Text'\n</code></pre>"},{"location":"config/#segmented-object-line","title":"Segmented object line","text":"Default:<pre><code>interlinear_mappings:\n  mb: 'Analyzed_Word'\n</code></pre>"},{"location":"config/#segmented-gloss-line","title":"Segmented gloss line","text":"Default:<pre><code>interlinear_mappings:\n  ge: 'Gloss'\n</code></pre>"},{"location":"config/#part-of-speech","title":"Part of speech","text":"Default:<pre><code>interlinear_mappings:\n  ps: 'Part_Of_Speech'\n</code></pre>"},{"location":"config/#translation","title":"Translation","text":"Default:<pre><code>interlinear_mappings:\n  ft: 'Translated_Text'\n</code></pre>"},{"location":"config/#lexicon-entry-marker","title":"Lexicon entry marker","text":"<p>The field holding lexicon entry identifiers.</p> Default:<pre><code>entry_marker: 'lx'\n</code></pre>"},{"location":"config/#mapping-lexicon-fields-to-columns","title":"Mapping lexicon fields to columns","text":"<p>The fields listed here will be renamed accordingly.</p>"},{"location":"config/#headword","title":"Headword","text":"Default:<pre><code>lexicon_mappings:\n  lx: 'Headword'\n</code></pre>"},{"location":"config/#part-of-speech_1","title":"Part of speech","text":"Default:<pre><code>lexicon_mappings:\n  ps: 'Part_Of_Speech'\n</code></pre>"},{"location":"config/#meaning","title":"Meaning","text":"Default:<pre><code>lexicon_mappings:\n  ge: 'Meaning'\n</code></pre>"},{"location":"config/#date","title":"Date","text":"Default:<pre><code>lexicon_mappings:\n  dt: 'Date'\n</code></pre>"},{"location":"config/#variants","title":"Variants","text":"Default:<pre><code>lexicon_mappings:\n  a: 'Variants'\n</code></pre>"},{"location":"config/#example-ids","title":"Example IDs","text":"<p>As suggested by Dictionaria.</p> Default:<pre><code>lexicon_mappings:\n  xref: 'Example_IDs'\n</code></pre>"},{"location":"config/#text-information","title":"Text information","text":"<p>Stored in <code>record_marker</code>, as a <code>separate</code> entry, or <code>none</code>?</p> Default:<pre><code>text_mode: 'none'\n</code></pre>"},{"location":"config/#shoebox","title":"Shoebox","text":"<p>File: <code>shoebox.yaml</code>.</p> <p>This is the default configuration for shoebox projects.</p>"},{"location":"config/#file-encoding_1","title":"File encoding","text":"<p>The default is the most frequent single-byte character encoding.  Other values I've had to use: <code>cp1256</code>, <code>iso8859_2</code>, <code>latin_1</code>.</p> Default:<pre><code>encoding: 'cp1252'\n</code></pre>"},{"location":"config/#text-record-marker_1","title":"Text record marker","text":"<p>The field holding text record identifiers.</p> Default:<pre><code>record_marker: 'ref'\n</code></pre>"},{"location":"config/#mapping-interlinear-fields-to-columns_1","title":"Mapping interlinear fields to columns","text":"<p>The fields listed here will be renamed accordingly.</p>"},{"location":"config/#unsegmented-object-line_1","title":"Unsegmented object line","text":"<p>The unsegmented first line of the record.</p> Default:<pre><code>interlinear_mappings:\n  t: 'Primary_Text'\n</code></pre>"},{"location":"config/#segmented-object-line_1","title":"Segmented object line","text":"Default:<pre><code>interlinear_mappings:\n  m: 'Analyzed_Word'\n</code></pre>"},{"location":"config/#segmented-gloss-line_1","title":"Segmented gloss line","text":"Default:<pre><code>interlinear_mappings:\n  g: 'Gloss'\n</code></pre>"},{"location":"config/#part-of-speech_2","title":"Part of speech","text":"Default:<pre><code>interlinear_mappings:\n  p: 'Part_Of_Speech'\n</code></pre>"},{"location":"config/#translation_1","title":"Translation","text":"Default:<pre><code>interlinear_mappings:\n  f: 'Translated_Text'\n</code></pre>"},{"location":"config/#comment","title":"Comment","text":"Default:<pre><code>interlinear_mappings:\n  c: 'Comment'\n</code></pre>"},{"location":"config/#lexicon-entry-marker_1","title":"Lexicon entry marker","text":"<p>The field holding lexicon entry identifiers.</p> Default:<pre><code>entry_marker: 'lx'\n</code></pre>"},{"location":"config/#mapping-lexicon-fields-to-columns_1","title":"Mapping lexicon fields to columns","text":"<p>The fields listed here will be renamed accordingly.</p>"},{"location":"config/#headword_1","title":"Headword","text":"Default:<pre><code>lexicon_mappings:\n  lx: 'Headword'\n</code></pre>"},{"location":"config/#part-of-speech_3","title":"Part of speech","text":"Default:<pre><code>lexicon_mappings:\n  ps: 'Part_Of_Speech'\n</code></pre>"},{"location":"config/#meaning_1","title":"Meaning","text":"Default:<pre><code>lexicon_mappings:\n  ge: 'Meaning'\n</code></pre>"},{"location":"config/#example-ids_1","title":"Example IDs","text":"<p>As suggested by Dictionaria.</p> Default:<pre><code>lexicon_mappings:\n  xref: 'Example_IDs'\n</code></pre>"},{"location":"config/#text-information_1","title":"Text information","text":"<p>Stored in <code>record_marker</code>, as a <code>separate</code> entry, or <code>none</code>?</p> Default:<pre><code>text_mode: 'none'\n</code></pre>"},{"location":"faq/","title":"FAQ","text":""},{"location":"usage/","title":"Usage","text":"<p>There are three CLI commands available, called with <code>unbox &lt;COMMAND&gt;</code>:</p> <ul> <li>corpus</li> <li>dictionary</li> <li>wordlist</li> </ul>"},{"location":"usage/#corpus","title":"corpus","text":"<p>Usage:</p> <pre><code>corpus [OPTIONS] [FILENAMES]...\n</code></pre> <p>Options:</p> <pre><code>  -o, --output PATH               Output directory  [default: .]\n  -c, --conf PATH                 Path to a yaml configuration file\n  -d, --cldf                      Create a CLDF dataset\n  -f, --format [toolbox|shoebox]  The format of the database you are\n                                  processing  [default: toolbox]\n  -a, --audio PATH                A directory containing your audio files.\n  -L, --languages PATH            A CSV file containing language data.\n  -s, --segments PATH             A CSV file mapping IPA to Graphemes.\n  -I, --include PATH              A yaml file with a list of allowed entries.\n  -i, --inflection PATH...        1. A CSV table of inflection categories. 2.\n                                  A CSV table of inflection values. 3. A .yaml\n                                  file with a dict mapping morph IDs to\n                                  inflectional values\n  -p, --parsing PATH              A parsing database.\n  -l, --lexicon PATH              Connect corpus to a lexicon\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"usage/#dictionary","title":"dictionary","text":"<p>Usage:</p> <pre><code>dictionary [OPTIONS] FILENAME\n</code></pre> <p>Options:</p> <pre><code>  -o, --output PATH               Output directory  [default: .]\n  -c, --conf PATH                 Path to a yaml configuration file\n  -d, --cldf                      Create a CLDF dataset\n  -f, --format [toolbox|shoebox]  The format of the database you are\n                                  processing  [default: toolbox]\n  -a, --audio PATH                A directory containing your audio files.\n  -L, --languages PATH            A CSV file containing language data.\n  -s, --segments PATH             A CSV file mapping IPA to Graphemes.\n  -I, --include PATH              A yaml file with a list of allowed entries.\n  -e, --examples PATH             An examples (corpus) database file\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"usage/#wordlist","title":"wordlist","text":"<p>Usage:</p> <pre><code>wordlist [OPTIONS] FILENAME\n</code></pre> <p>Options:</p> <pre><code>  -o, --output PATH               Output directory  [default: .]\n  -c, --conf PATH                 Path to a yaml configuration file\n  -d, --cldf                      Create a CLDF dataset\n  -f, --format [toolbox|shoebox]  The format of the database you are\n                                  processing  [default: toolbox]\n  -a, --audio PATH                A directory containing your audio files.\n  -L, --languages PATH            A CSV file containing language data.\n  -s, --segments PATH             A CSV file mapping IPA to Graphemes.\n  -I, --include PATH              A yaml file with a list of allowed entries.\n  --help                          Show this message and exit.\n</code></pre>"}]}